{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82ad0837",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39326dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.conda/envs/esie/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import time\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcdab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/llama-2-7b-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4fbb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10 # possible sparsity levels (0.0 - 0.9)\n",
    "S = 3 # number of state features\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.base = nn.Sequential(\n",
    "            nn.Linear(S+N, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.head = nn.Linear(256, N)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base(x)\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, model_name, num_samples, sequence_length, target_sparsity=0.5):\n",
    "        self.model_name = model_name\n",
    "        self.num_samples = num_samples\n",
    "        self.sequence_length = sequence_length\n",
    "        self.target_sparsity = target_sparsity\n",
    "        self.num_samples = num_samples\n",
    "        self.sequence_length = sequence_length\n",
    "        self.possible_sparsities = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # initialize state\n",
    "        self.reset()\n",
    "        self.pruning_info = {}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def init(self):\n",
    "        # create model, tokenizer, and calibration data.\n",
    "        # model and tokenizer\n",
    "        model = AutoModelForCausalLM.from_pretrained(self.model_name, dtype=torch.float16, device_map=\"auto\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # caliberation data\n",
    "        num_tokens = self.num_samples * self.sequence_length\n",
    "        calib_data = get_fineweb_edu(num_tokens, self.sequence_length, tokenizer, train=True)\n",
    "        test_data = get_fineweb_edu(num_tokens, self.sequence_length, tokenizer, train=False)\n",
    "\n",
    "        # env attributes\n",
    "        self.action_mask = torch.ones(N)\n",
    "        self.layers = model.model.layers\n",
    "        self.num_layers = len(self.layers)\n",
    "        self.current_layer = 0\n",
    "        self.global_sparsity = 0.0\n",
    "        self.layer_sparsities = [0.0] * self.num_layers\n",
    "        self.calib_data = calib_data\n",
    "\n",
    "        # buffers\n",
    "        self.inps = torch.zeros((self.num_samples, self.sequence_length, model.config.hidden_size), dtype=torch.float16, device=self.device)\n",
    "        self.outs = torch.zeros_like(self.inps)\n",
    "        self.inp_kwargs = {}\n",
    "\n",
    "        # obtain input into the first decoder layer\n",
    "        cache = model.config.use_cache\n",
    "        model.config.use_cache = False\n",
    "        inps = self.inps\n",
    "        inp_kwargs = self.inp_kwargs\n",
    "        class catch_inps(nn.Module):\n",
    "            def __init__(self, module):\n",
    "                super().__init__()\n",
    "                self.module = module\n",
    "                self.num_inps = 0\n",
    "            def forward(self, inp, **kwargs):\n",
    "                nonlocal inps, inp_kwargs\n",
    "                inps[self.num_inps] = inp\n",
    "                inp_kwargs.update(kwargs)\n",
    "                self.num_inps += 1\n",
    "                raise Exception(\"caught inps. Stopping forward pass.\")\n",
    "        self.layers[0] = catch_inps(self.layers[0])\n",
    "        for sample in self.calib_data:\n",
    "            try:\n",
    "                model(sample.to(self.device))\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        self.layers[0] = self.layers[0].module\n",
    "        self.inps = inps\n",
    "        self.inp_kwargs = inp_kwargs\n",
    "\n",
    "        # save the log targets to a file for computing the KL divergence later\n",
    "        i_batches = 0\n",
    "        os.makedirs(f\"logs/kl/{self.model_name}\", exist_ok=True)\n",
    "        batch_size = 4\n",
    "        log_probs = []\n",
    "        for j in range(self.num_samples):\n",
    "            if os.path.exists(f\"logs/kl/{self.model_name}/log_targets_{(j//batch_size)}_{batch_size}.pt\"):\n",
    "                i_batches = j // batch_size\n",
    "                continue\n",
    "            sample = test_data[j]\n",
    "            logits = model(sample.to(self.device)).logits\n",
    "            log_probs.append(F.log_softmax(logits.float(), dim=-1).reshape(-1, model.config.vocab_size).cpu())\n",
    "            if j % batch_size == batch_size-1:\n",
    "                log_probs = torch.cat(log_probs, dim=0).cpu()\n",
    "                torch.save(log_probs, f\"logs/{self.model_name}/log_targets_{i_batches}_{batch_size}.pt\")\n",
    "                log_probs = []\n",
    "            elif j == self.num_samples - 1 and len(log_probs) > 0:\n",
    "                log_probs = torch.cat(log_probs, dim=0).cpu()\n",
    "                torch.save(log_probs, f\"logs/{self.model_name}/log_targets_{i_batches}_{batch_size}.pt\")\n",
    "            i_batches = j // batch_size\n",
    "        print(f\"Saved {i_batches+1} batches of log probabilities for KL divergence computation.\")\n",
    "        # create a dataloader for computing KL divergence later\n",
    "        model_name = self.model_name\n",
    "        class KLDataset(torch.utils.data.Dataset):\n",
    "            def __init__(self):\n",
    "                self.path_format = f\"logs/{model_name}\"+\"/log_targets_{}_{}.pt\"\n",
    "            def __len__(self):\n",
    "                return i_batches + 1\n",
    "            def __getitem__(self, idx):\n",
    "                nonlocal batch_size\n",
    "                samples = torch.cat(test_data[idx*batch_size:(idx+1)*batch_size], dim=0)\n",
    "                log_probs = torch.load(self.path_format.format(idx, batch_size))\n",
    "                return samples, log_probs\n",
    "        self.kl_dataloader = torch.utils.data.DataLoader(KLDataset(), batch_size=1, shuffle=False)\n",
    "        print(f\"KL dataloader with {len(self.kl_dataloader)} batches created.\")\n",
    "        model.config.use_cache = cache\n",
    "\n",
    "    def prune_layer(self, layer_idx, sparsity):\n",
    "        if layer_idx in self.pruning_info:\n",
    "            raise Exception(f\"Layer {layer_idx} already pruned. Skipping.\")\n",
    "        \n",
    "        layer = self.layers[layer_idx]\n",
    "        sublayers = {name: module for name, module in layer.named_modules() if isinstance(module, nn.Linear)}\n",
    "        wrapped_layers = {}\n",
    "        for name, sublayer in sublayers.items():\n",
    "            wrapped_layers[name] = WrappedGPT(sublayer)\n",
    "\n",
    "        # obtain the input activations to each sublayer, computing the feature-wise norms\n",
    "        def add_batch(name):\n",
    "            def tmp(_, inp, out):\n",
    "                wrapped_layers[name].add_batch(inp[0].data, out.data)\n",
    "            return tmp\n",
    "        handles = []\n",
    "        for name in wrapped_layers:\n",
    "            handles.append(sublayers[name].register_forward_hook(add_batch(name)))\n",
    "        for j in range(self.num_samples):\n",
    "            self.outs[j] = layer(self.inps[j].unsqueeze(0), **self.inp_kwargs)[0]\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "        \n",
    "        for name in sublayers:\n",
    "            wrapped_layers[name].prune(sparsity)\n",
    "            wrapped_layers[name].clean()\n",
    "\n",
    "        # outputs after pruning\n",
    "        for j in range(self.num_samples):\n",
    "            with torch.no_grad():\n",
    "                self.outs[j] = layer(self.inps[j].unsqueeze(0), **self.inp_kwargs)[0]\n",
    "\n",
    "        # the output from this layer should be the input to the next layer\n",
    "        self.inps, self.outs = self.outs, self.inps\n",
    "\n",
    "        # done pruning this layer. Prepare some info about this layer's pruning\n",
    "        obtained_sparsity = np.mean([l.weight.data.eq(0).float().mean().item() for l in sublayers.values()]).item()\n",
    "        info = {\n",
    "            \"layer\": layer_idx,\n",
    "            \"layer_target_sparsity\": sparsity,\n",
    "            \"layer_obtained_sparsity\": obtained_sparsity,\n",
    "        }\n",
    "        self.pruning_info[layer_idx] = info\n",
    "\n",
    "    def reset(self):\n",
    "        if hasattr(self, \"inps\"):\n",
    "            del self.inps, self.outs, self.inp_kwargs\n",
    "            del self.kl_dataloader\n",
    "            del self.model, self.tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "        self.init()\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        s = [self.global_sparsity, self.target_sparsity, self.current_layer / self.num_layers]\n",
    "        if self.current_layer == 0:\n",
    "            mask = [1] * len(self.possible_sparsities)\n",
    "        else:\n",
    "            mask = [1 if (sum(self.layer_sparsities[:self.current_layer]) + s) / self.current_layer <= self.target_sparsity else 0 for s in self.possible_sparsities]\n",
    "        state = {\n",
    "            \"state\": torch.tensor(s, dtype=torch.float32),\n",
    "            \"action_mask\": torch.tensor(mask, dtype=torch.float32)\n",
    "        }\n",
    "        return state\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, action):\n",
    "        sparsity = self.possible_sparsities[action]\n",
    "        self.prune_layer(self.current_layer, sparsity)\n",
    "        # update global sparsity\n",
    "        self.layer_sparsities[self.current_layer] = sparsity\n",
    "        self.current_layer += 1\n",
    "        self.global_sparsity = np.mean(self.layer_sparsities[:self.current_layer])\n",
    "        # compute reward\n",
    "        reward = 0\n",
    "        done = self.current_layer == self.num_layers\n",
    "        if done:\n",
    "            # compute KL divergence between the pruned and unpruned model.\n",
    "            # the logits have been saved to a file during initialization.\n",
    "            running_kl = 0.0\n",
    "            total_logprobs = 0\n",
    "            for batch in self.kl_dataloader:\n",
    "                inps, target_log_probs = [batch[0].squeeze(0), batch[1].squeeze(0)]\n",
    "                logits = self.model(inps.to(self.device)).logits.reshape(-1, self.model.config.vocab_size)\n",
    "                log_probs = F.log_softmax(logits.float(), dim=-1)\n",
    "                kl = F.kl_div(log_probs, target_log_probs.to(self.device), reduction=\"batchmean\", log_target=True).item()\n",
    "                running_kl *= (total_logprobs / (total_logprobs + target_log_probs.numel()))\n",
    "                running_kl += (target_log_probs.numel() / (total_logprobs + target_log_probs.numel())) * kl\n",
    "                total_logprobs += target_log_probs.numel()\n",
    "                del target_log_probs, logits, kl\n",
    "                torch.cuda.empty_cache()\n",
    "            reward = -running_kl\n",
    "\n",
    "        return self.get_state(), reward, done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69da1883",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FineWeb-Edu v2\n",
      "Total tokens loaded: 524288\n",
      "Loading FineWeb-Edu v2\n",
      "Total tokens loaded: 524288\n",
      "Saved 32 batches of log probabilities for KL divergence computation.\n",
      "KL dataloader with 32 batches created.\n"
     ]
    }
   ],
   "source": [
    "environment = Environment(model_name, num_samples=128, sequence_length=4096, target_sparsity=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b1c03e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FineWeb-Edu v2\n",
      "Total tokens loaded: 524288\n",
      "Loading FineWeb-Edu v2\n",
      "Total tokens loaded: 524288\n",
      "Saved 32 batches of log probabilities for KL divergence computation.\n",
      "KL dataloader with 32 batches created.\n",
      "Layer: 0, Action: 5, Reward: 0, Done: False\n",
      "Layer: 1, Action: 5, Reward: 0, Done: False\n",
      "Layer: 2, Action: 5, Reward: 0, Done: False\n",
      "Layer: 3, Action: 5, Reward: 0, Done: False\n",
      "Layer: 4, Action: 5, Reward: 0, Done: False\n",
      "Layer: 5, Action: 5, Reward: 0, Done: False\n",
      "Layer: 6, Action: 5, Reward: 0, Done: False\n",
      "Layer: 7, Action: 5, Reward: 0, Done: False\n",
      "Layer: 8, Action: 5, Reward: 0, Done: False\n",
      "Layer: 9, Action: 5, Reward: 0, Done: False\n",
      "Layer: 10, Action: 5, Reward: 0, Done: False\n",
      "Layer: 11, Action: 5, Reward: 0, Done: False\n",
      "Layer: 12, Action: 5, Reward: 0, Done: False\n",
      "Layer: 13, Action: 5, Reward: 0, Done: False\n",
      "Layer: 14, Action: 5, Reward: 0, Done: False\n",
      "Layer: 15, Action: 5, Reward: 0, Done: False\n",
      "Layer: 16, Action: 5, Reward: 0, Done: False\n",
      "Layer: 17, Action: 5, Reward: 0, Done: False\n",
      "Layer: 18, Action: 5, Reward: 0, Done: False\n",
      "Layer: 19, Action: 5, Reward: 0, Done: False\n",
      "Layer: 20, Action: 5, Reward: 0, Done: False\n",
      "Layer: 21, Action: 5, Reward: 0, Done: False\n",
      "Layer: 22, Action: 5, Reward: 0, Done: False\n",
      "Layer: 23, Action: 5, Reward: 0, Done: False\n",
      "Layer: 24, Action: 5, Reward: 0, Done: False\n",
      "Layer: 25, Action: 5, Reward: 0, Done: False\n",
      "Layer: 26, Action: 5, Reward: 0, Done: False\n",
      "Layer: 27, Action: 5, Reward: 0, Done: False\n",
      "Layer: 28, Action: 5, Reward: 0, Done: False\n",
      "Layer: 29, Action: 5, Reward: 0, Done: False\n",
      "Layer: 30, Action: 5, Reward: 0, Done: False\n",
      "Layer: 31, Action: 5, Reward: -0.21203863481059673, Done: True\n"
     ]
    }
   ],
   "source": [
    "state = environment.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = 5\n",
    "    lidx = environment.current_layer\n",
    "    state, reward, done, _ = environment.step(action)\n",
    "    print(f\"Layer: {lidx}, Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee32bd1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'layer': 0, 'layer_target_sparsity': 0.5, 'layer_obtained_sparsity': 0.5},\n",
       " 1: {'layer': 1, 'layer_target_sparsity': 0.5, 'layer_obtained_sparsity': 0.5},\n",
       " 2: {'layer': 2, 'layer_target_sparsity': 0.5, 'layer_obtained_sparsity': 0.5},\n",
       " 3: {'layer': 3, 'layer_target_sparsity': 0.5, 'layer_obtained_sparsity': 0.5},\n",
       " 4: {'layer': 4, 'layer_target_sparsity': 0.5, 'layer_obtained_sparsity': 0.5},\n",
       " 5: {'layer': 5, 'layer_target_sparsity': 0.5, 'layer_obtained_sparsity': 0.5},\n",
       " 6: {'layer': 6, 'layer_target_sparsity': 0.5, 'layer_obtained_sparsity': 0.5},\n",
       " 7: {'layer': 7, 'layer_target_sparsity': 0.5, 'layer_obtained_sparsity': 0.5},\n",
       " 8: {'layer': 8, 'layer_target_sparsity': 0.5, 'layer_obtained_sparsity': 0.5},\n",
       " 9: {'layer': 9, 'layer_target_sparsity': 0.5, 'layer_obtained_sparsity': 0.5},\n",
       " 10: {'layer': 10,\n",
       "  'layer_target_sparsity': 0.5,\n",
       "  'layer_obtained_sparsity': 0.5},\n",
       " 11: {'layer': 11,\n",
       "  'layer_target_sparsity': 0.5,\n",
       "  'layer_obtained_sparsity': 0.5},\n",
       " 12: {'layer': 12,\n",
       "  'layer_target_sparsity': 0.5,\n",
       "  'layer_obtained_sparsity': 0.5},\n",
       " 13: {'layer': 13,\n",
       "  'layer_target_sparsity': 0.5,\n",
       "  'layer_obtained_sparsity': 0.5},\n",
       " 14: {'layer': 14,\n",
       "  'layer_target_sparsity': 0.5,\n",
       "  'layer_obtained_sparsity': 0.5},\n",
       " 15: {'layer': 15,\n",
       "  'layer_target_sparsity': 0.5,\n",
       "  'layer_obtained_sparsity': 0.5},\n",
       " 16: {'layer': 16,\n",
       "  'layer_target_sparsity': 0.5,\n",
       "  'layer_obtained_sparsity': 0.5},\n",
       " 17: {'layer': 17,\n",
       "  'layer_target_sparsity': 0.5,\n",
       "  'layer_obtained_sparsity': 0.5},\n",
       " 18: {'layer': 18,\n",
       "  'layer_target_sparsity': 0.5,\n",
       "  'layer_obtained_sparsity': 0.5},\n",
       " 19: {'layer': 19,\n",
       "  'layer_target_sparsity': 0.5,\n",
       "  'layer_obtained_sparsity': 0.5},\n",
       " 20: {'layer': 20,\n",
       "  'layer_target_sparsity': 0.5,\n",
       "  'layer_obtained_sparsity': 0.5},\n",
       " 21: {'layer': 21,\n",
       "  'layer_target_sparsity': 0.5,\n",
       "  'layer_obtained_sparsity': 0.5},\n",
       " 22: {'layer': 22,\n",
       "  'layer_target_sparsity': 0.5,\n",
       "  'layer_obtained_sparsity': 0.5},\n",
       " 23: {'layer': 23,\n",
       "  'layer_target_sparsity': 0.5,\n",
       "  'layer_obtained_sparsity': 0.5},\n",
       " 24: {'layer': 24,\n",
       "  'layer_target_sparsity': 0.5,\n",
       "  'layer_obtained_sparsity': 0.5},\n",
       " 25: {'layer': 25,\n",
       "  'layer_target_sparsity': 0.5,\n",
       "  'layer_obtained_sparsity': 0.5},\n",
       " 26: {'layer': 26,\n",
       "  'layer_target_sparsity': 0.5,\n",
       "  'layer_obtained_sparsity': 0.5},\n",
       " 27: {'layer': 27,\n",
       "  'layer_target_sparsity': 0.5,\n",
       "  'layer_obtained_sparsity': 0.5},\n",
       " 28: {'layer': 28,\n",
       "  'layer_target_sparsity': 0.5,\n",
       "  'layer_obtained_sparsity': 0.5},\n",
       " 29: {'layer': 29,\n",
       "  'layer_target_sparsity': 0.5,\n",
       "  'layer_obtained_sparsity': 0.5},\n",
       " 30: {'layer': 30,\n",
       "  'layer_target_sparsity': 0.5,\n",
       "  'layer_obtained_sparsity': 0.5},\n",
       " 31: {'layer': 31,\n",
       "  'layer_target_sparsity': 0.5,\n",
       "  'layer_obtained_sparsity': 0.5}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.pruning_info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
