{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3af39dea-f539-4394-bd62-21c9678c54d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a00e147-3b23-4cb9-b9b4-4510038e2e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fcc3390-34e5-410b-9f15-ff4f2d7c02ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84b49fb9-c614-4b39-b5cd-7bdaabbbd1cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb242a26cd9407fbba959abf5227435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0202ad0edce14d02b7450e1b506396fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "# Calibration datasets\n",
    "NUM_SAMPLES = 4\n",
    "SEQUENCE_LEN = 1024\n",
    "finewebedu_dataset = load_dataset(\"HuggingFaceFW/fineweb-edu\", \"sample-10BT\", split=\"train\")\n",
    "c4_train_dataset = load_dataset(\n",
    "            \"allenai/c4\",\n",
    "            \"default\",\n",
    "            data_files={\"train\": \"en/c4-train.00000-of-01024.json.gz\"},\n",
    "            split=\"train\",\n",
    "            revision=\"607bd4c8450a42878aa9ddc051a65a055450ef87\",  # pin revision\n",
    "        )\n",
    "c4_val_dataset = load_dataset(\n",
    "            \"allenai/c4\",\n",
    "            \"default\",\n",
    "            data_files={\"validation\": \"en/c4-validation.00000-of-00008.json.gz\"},\n",
    "            split=\"validation[:1100]\",\n",
    "            revision=\"607bd4c8450a42878aa9ddc051a65a055450ef87\",  # pin revision\n",
    "        )\n",
    "w2_train_dataset = dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "w2_val_dataset = dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbc54263-0ffc-4357-8596-d70ee9c3c0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "finewebedu_train_data = finewebedu_dataset.select(range(dataset.num_rows//2))\n",
    "finewebedu_val_data = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f7acfcca-0829-49f2-befb-b66f421d493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIKITEXT train and test tokens\n",
    "def get_w2_data(num_samples, seq_len, tokenizer):\n",
    "    w2_train_dataset = dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "    w2_val_dataset = dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "\n",
    "    train_tokens = tokenizer(\"\\n\\n\".join(w2_train_dataset[\"text\"]), return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "    test_tokens = tokenizer(\"\\n\\n\".join(w2_val_dataset[\"text\"]), return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "    \n",
    "    num_tokens = train_tokens.size(1)\n",
    "    idxes = np.random.choice(num_tokens-seq_len, num_samples, replace=False).tolist()\n",
    "    test_idxes = range(test_tokens.size(1)//seq_len)\n",
    "    \n",
    "    train_data = map(lambda idx: train_tokens[:, idx:idx+seq_len], idxes)\n",
    "    test_data = map(lambda idx: test_tokens[:, idx*seq_len:(idx+1)*seq_len], test_idxes)\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b66b9aef-2ed7-4df2-a56b-ee6a9a0aa7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C4 train and test tokens\n",
    "def get_c4_data(num_samples, seq_len, tokenizer):\n",
    "    c4_train_dataset = load_dataset(\"allenai/c4\", \"default\", data_files={\"train\": \"en/c4-train.00000-of-01024.json.gz\"},split=\"train\", revision=\"607bd4c8450a42878aa9ddc051a65a055450ef87\")\n",
    "    c4_val_dataset = load_dataset(\"allenai/c4\", \"default\", data_files={\"validation\": \"en/c4-validation.00000-of-00008.json.gz\"}, split=\"validation[:1100]\", revision=\"607bd4c8450a42878aa9ddc051a65a055450ef87\")\n",
    "\n",
    "    train_tokens = tokenizer(\"\\n\\n\".join(c4_train_dataset[\"text\"][:seq_len+10]), return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "    test_tokens = tokenizer(\"\\n\\n\".join(c4_val_dataset[\"text\"]), return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "    \n",
    "    num_tokens = train_tokens.size(1)\n",
    "    idxes = np.random.choice(num_tokens-seq_len, num_samples, replace=False).tolist()\n",
    "    test_idxes = range(test_tokens.size(1)//seq_len)\n",
    "    \n",
    "    train_data = list(map(lambda idx: train_tokens[:, idx:idx+seq_len], idxes))\n",
    "    test_data = list(map(lambda idx: test_tokens[:, idx*seq_len:(idx+1)*seq_len], test_idxes))\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8309cb-6acf-4d68-bdad-10aa5427ad4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finewebedu\n",
    "def get_fw_data(num_samples, seq_len, tokenizer):\n",
    "    fw_dataset = load_dataset(\"HuggingFaceFW/fineweb-edu\", \"sample-10BT\", split=\"train\")\n",
    "\n",
    "    ds_size = fw_dataset.num_samples\n",
    "    train_tokens = tokenizer(\"\\n\\n\".join(fw_train_dataset[\"text\"]), return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "    test_tokens = tokenizer(\"\\n\\n\".join(fw_val_dataset[\"text\"]), return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "    \n",
    "    num_tokens = train_tokens.size(1)\n",
    "    idxes = np.random.choice(num_tokens-seq_len, num_samples, replace=False).tolist()\n",
    "    test_idxes = range(test_tokens.size(1)//seq_len)\n",
    "    \n",
    "    train_data = map(lambda idx: train_tokens[:, idx:idx+seq_len], idxes)\n",
    "    test_data = map(lambda idx: test_tokens[:, idx*seq_len:(idx+1)*seq_len], test_idxes)\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3de7f9b0-a924-45ad-94c2-355fe5b4b2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fineweb:\n",
    "# Source: https://github.com/IST-DASLab/EvoPress/blob/main/src/data_utils.py\n",
    "def get_fineweb_edu(num_tokens, sequence_length, tokenizer, train = True):\n",
    "    print_on_main(\"Loading FineWeb-Edu v2\")\n",
    "    dataset = load_dataset(\"HuggingFaceFW/fineweb-edu\", \"sample-10BT\", split=\"train\")\n",
    "    tokens_to_load = num_tokens\n",
    "    if train:\n",
    "        dataset = dataset.select(range(dataset.num_rows//2))\n",
    "    else:\n",
    "        dataset = dataset.select(range(dataset.num_rows//2, dataset.num_rows))\n",
    "    dataset = dataset.shuffle(seed=0)\n",
    "    data_iter = iter(dataset)\n",
    "    data = []\n",
    "    while tokens_to_load > 0:\n",
    "        sample = next(data_iter)\n",
    "        tokenized_sample = tokenizer(sample[\"text\"], return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "        tokenized_sample = tokenized_sample[:, :min(tokenized_sample.shape[1], tokens_to_load)]\n",
    "        # Split the sequence into multiple samples if it is too long\n",
    "        # Just throwing away extra tokens would introduce bias to the dataset\n",
    "        while tokenized_sample.shape[1] > sequence_length:\n",
    "            data.append(tokenized_sample[:, :sequence_length])\n",
    "            tokenized_sample = tokenized_sample[:, sequence_length:]\n",
    "            tokens_to_load -= sequence_length\n",
    "        data.append(tokenized_sample)\n",
    "        tokens_to_load -= tokenized_sample.shape[1]\n",
    "    print_on_main(f\"Total tokens loaded: {sum([sample.shape[1] for sample in data])}\")\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4196448a-ec04-44d0-ab19-3919006cc95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2_train_data, w2_test_data = get_w2_data(NUM_SAMPLES, SEQUENCE_LEN, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "001f76cc-e1fc-47f2-ae8c-4387dc57896d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "c4_train_data, c4_test_data = get_c4_data(NUM_SAMPLES, SEQUENCE_LEN, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "99b48cae-3758-40f1-8bed-1c46ea3631bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b28d324522f4371b994fa807c01f758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "145cf363165c4153a01280c9bc5afe25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fw_dataset = load_dataset(\"HuggingFaceFW/fineweb-edu\", \"sample-10BT\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "196583af-9058-4a71-96f4-1102b02cc7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9672101"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fw_dataset.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a39d57b4-fd8d-4871-b30d-176c99f661cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mfw_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m9\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "fw_dataset[:4][\"text\"][9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa588739-97a6-4d8c-9d71-de25a7e958ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[48290,  7130, 22658,  ...,  5286,  3353,     4]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"\\n\\n\".join(c4_train_dataset[\"text\"][:50000]), return_tensors=\"pt\", add_special_tokens=False).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a0fa350-2f12-4343-b26d-d89d291d3a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c4_train_dataset[:100][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f107eae-bda6-4e34-9211-c210925a1117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
