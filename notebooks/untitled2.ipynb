{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87388c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abef8bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a276ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/esie/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Dict, Tuple\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "try:\n",
    "    import wandb\n",
    "except ImportError:\n",
    "    print(\"wandb not installed. WandBLogger will not work.\")\n",
    "    wandb = None\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5debb54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Environment:\n",
    "    def __init__(self, model_name:str, num_samples:int, sequence_length:int, target_sparsity:float=0.5)->None:\n",
    "        self.model_name = model_name\n",
    "        self.num_samples = num_samples\n",
    "        self.sequence_length = sequence_length\n",
    "        self.target_sparsity = target_sparsity\n",
    "        self.num_samples = num_samples\n",
    "        self.sequence_length = sequence_length\n",
    "        self.possible_sparsities = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "        self.device = device if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # initialize state\n",
    "        self.load_calibration_data()\n",
    "        self.reset()\n",
    "\n",
    "    def load_calibration_data(self):\n",
    "        # caliberation data\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        self.tokenizer = tokenizer\n",
    "        num_tokens = self.num_samples * self.sequence_length\n",
    "        self.calib_data = get_fineweb_edu(num_tokens, self.sequence_length, tokenizer, train=True)\n",
    "        # self.test_data = get_fineweb_edu(num_tokens, self.sequence_length, tokenizer, train=False)\n",
    "        _, self.test_data = get_w2_data(self.num_samples, self.sequence_length, tokenizer)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def init(self) -> None:\n",
    "        # create model, tokenizer, and calibration data.\n",
    "        # model and tokenizer\n",
    "        model = AutoModelForCausalLM.from_pretrained(self.model_name, dtype=torch.float16, device_map=\"auto\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # caliberation data\n",
    "        test_data = self.test_data\n",
    "\n",
    "        # env attributes\n",
    "        self.action_mask = torch.ones(N)\n",
    "        self.layers = model.model.layers\n",
    "        self.num_layers = len(self.layers)\n",
    "        self.current_layer = 0\n",
    "        self.global_sparsity = 0.0\n",
    "        self.layer_sparsities = [0.0] * self.num_layers\n",
    "        self.pruning_info = {}\n",
    "\n",
    "        # buffers\n",
    "        self.inps = torch.zeros((self.num_samples, self.sequence_length, model.config.hidden_size), dtype=torch.float16, device=self.device)\n",
    "        self.outs = torch.zeros_like(self.inps)\n",
    "        self.inp_kwargs = {}\n",
    "\n",
    "        # obtain input into the first decoder layer\n",
    "        cache = model.config.use_cache\n",
    "        model.config.use_cache = False\n",
    "        inps = self.inps\n",
    "        inp_kwargs = self.inp_kwargs\n",
    "        class catch_inps(nn.Module):\n",
    "            def __init__(self, module):\n",
    "                super().__init__()\n",
    "                self.module = module\n",
    "                self.num_inps = 0\n",
    "            def forward(self, inp, **kwargs):\n",
    "                nonlocal inps, inp_kwargs\n",
    "                inps[self.num_inps] = inp\n",
    "                inp_kwargs.update(kwargs)\n",
    "                self.num_inps += 1\n",
    "                raise Exception(\"caught inps. Stopping forward pass.\")\n",
    "        self.layers[0] = catch_inps(self.layers[0])\n",
    "        for sample in self.calib_data:\n",
    "            try:\n",
    "                model(sample.to(self.device))\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        self.layers[0] = self.layers[0].module\n",
    "        self.inps = inps\n",
    "        self.inp_kwargs = inp_kwargs\n",
    "\n",
    "        # save the log targets to a file for computing the KL divergence later\n",
    "        i_batches = 0\n",
    "        os.makedirs(f\"logs/kl/{self.model_name}\", exist_ok=True)\n",
    "        batch_size = 4\n",
    "        log_probs = []\n",
    "        for j in range(self.num_samples):\n",
    "            if os.path.exists(f\"logs/kl/{self.model_name}/log_targets_{(j//batch_size)}_{batch_size}.pt\"):\n",
    "                i_batches = j // batch_size\n",
    "                continue\n",
    "            sample = test_data[j]\n",
    "            logits = model(sample.to(self.device)).logits\n",
    "            log_probs.append(F.log_softmax(logits.float(), dim=-1).reshape(-1, model.config.vocab_size).cpu())\n",
    "            if j % batch_size == batch_size-1:\n",
    "                log_probs = torch.cat(log_probs, dim=0).cpu()\n",
    "                torch.save(log_probs, f\"logs/kl/{self.model_name}/log_targets_{i_batches}_{batch_size}.pt\")\n",
    "                print(f\"Saved logs/kl/{self.model_name}/log_targets_{i_batches}_{batch_size}.pt\")\n",
    "                log_probs = []\n",
    "            elif j == self.num_samples - 1 and len(log_probs) > 0:\n",
    "                log_probs = torch.cat(log_probs, dim=0).cpu()\n",
    "                torch.save(log_probs, f\"logs/kl/{self.model_name}/log_targets_{i_batches}_{batch_size}.pt\")\n",
    "                print(f\"Saved logs/kl/{self.model_name}/log_targets_{i_batches}_{batch_size}.pt\")\n",
    "            i_batches = j // batch_size\n",
    "            \n",
    "        # create a dataloader for computing KL divergence later\n",
    "        model_name = self.model_name\n",
    "        class KLDataset(torch.utils.data.Dataset):\n",
    "            def __init__(self):\n",
    "                self.path_format = f\"logs/kl/{model_name}\"+\"/log_targets_{}_{}.pt\"\n",
    "            def __len__(self):\n",
    "                return i_batches + 1\n",
    "            def __getitem__(self, idx):\n",
    "                nonlocal batch_size\n",
    "                samples = torch.cat(test_data[idx*batch_size:(idx+1)*batch_size], dim=0)\n",
    "                log_probs = torch.load(self.path_format.format(idx, batch_size))\n",
    "                return samples, log_probs\n",
    "        self.kl_dataloader = torch.utils.data.DataLoader(KLDataset(), batch_size=1, shuffle=False)\n",
    "        # print(f\"KL dataloader with {len(self.kl_dataloader)} batches created.\")\n",
    "        model.config.use_cache = cache\n",
    "\n",
    "    def prune_layer(self, layer_idx:int, sparsity:float)->None:\n",
    "        if layer_idx in self.pruning_info:\n",
    "            raise Exception(f\"Layer {layer_idx} already pruned. Skipping.\")\n",
    "        \n",
    "        layer = self.layers[layer_idx]\n",
    "        sublayers = {name: module for name, module in layer.named_modules() if isinstance(module, nn.Linear)}\n",
    "        wrapped_layers = {}\n",
    "        for name, sublayer in sublayers.items():\n",
    "            wrapped_layers[name] = WrappedGPT(sublayer)\n",
    "\n",
    "        # obtain the input activations to each sublayer, computing the feature-wise norms\n",
    "        def add_batch(name):\n",
    "            def tmp(_, inp, out):\n",
    "                wrapped_layers[name].add_batch(inp[0].data, out.data)\n",
    "            return tmp\n",
    "        handles = []\n",
    "        for name in wrapped_layers:\n",
    "            handles.append(sublayers[name].register_forward_hook(add_batch(name)))\n",
    "        for j in range(self.num_samples):\n",
    "            self.outs[j] = layer(self.inps[j].unsqueeze(0), **self.inp_kwargs)[0]\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "        \n",
    "        for name in sublayers:\n",
    "            wrapped_layers[name].prune(sparsity)\n",
    "            wrapped_layers[name].clean()\n",
    "\n",
    "        # outputs after pruning\n",
    "        for j in range(self.num_samples):\n",
    "            with torch.no_grad():\n",
    "                self.outs[j] = layer(self.inps[j].unsqueeze(0), **self.inp_kwargs)[0]\n",
    "\n",
    "        # the output from this layer should be the input to the next layer\n",
    "        self.inps, self.outs = self.outs, self.inps\n",
    "\n",
    "        # done pruning this layer. Prepare some info about this layer's pruning\n",
    "        obtained_sparsity = np.mean([l.weight.data.eq(0).float().mean().item() for l in sublayers.values()]).item()\n",
    "        info = {\n",
    "            \"layer\": layer_idx,\n",
    "            \"layer_target_sparsity\": sparsity,\n",
    "            \"layer_obtained_sparsity\": obtained_sparsity,\n",
    "        }\n",
    "        self.pruning_info[layer_idx] = info\n",
    "\n",
    "    def reset(self) -> Dict[str, torch.Tensor]:\n",
    "        if hasattr(self, \"inps\"):\n",
    "            del self.inps, self.outs, self.inp_kwargs\n",
    "            del self.kl_dataloader\n",
    "            del self.model, self.tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "        self.init()\n",
    "        return self.get_state(), {}\n",
    "\n",
    "    def get_state(self) -> Dict[str, torch.Tensor]:\n",
    "        s = [self.global_sparsity, self.target_sparsity, self.current_layer / self.num_layers]\n",
    "        if self.current_layer == 0:\n",
    "            mask = [1] * len(self.possible_sparsities)\n",
    "        else:\n",
    "            mask = [1 if (sum(self.layer_sparsities[:self.current_layer]) + s) / self.current_layer <= self.target_sparsity else 0 for s in self.possible_sparsities]\n",
    "        state = {\n",
    "            \"state\": torch.tensor(s, dtype=torch.float32),\n",
    "            \"action_mask\": torch.tensor(mask, dtype=torch.float32)\n",
    "        }\n",
    "        return state\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, action:int)->Tuple[Dict[str, torch.Tensor], float, bool, Dict[str, object]]:\n",
    "        sparsity = self.possible_sparsities[action]\n",
    "        self.prune_layer(self.current_layer, sparsity)\n",
    "        # update global sparsity\n",
    "        self.layer_sparsities[self.current_layer] = sparsity\n",
    "        self.current_layer += 1\n",
    "        self.global_sparsity = np.mean(self.layer_sparsities[:self.current_layer])\n",
    "        # compute reward\n",
    "        reward = 0\n",
    "        done = self.current_layer == self.num_layers\n",
    "        if done:\n",
    "            # compute KL divergence between the pruned and unpruned model.\n",
    "            # the logits have been saved to a file during initialization.\n",
    "            running_kl = 0.0\n",
    "            total_logprobs = 0\n",
    "            # for batch in self.kl_dataloader:\n",
    "            #     inps, target_log_probs = [batch[0].squeeze(0), batch[1].squeeze(0)]\n",
    "            #     logits = self.model(inps.to(self.device)).logits.reshape(-1, self.model.config.vocab_size)\n",
    "            #     log_probs = F.log_softmax(logits.float(), dim=-1)\n",
    "            #     kl = F.kl_div(log_probs, target_log_probs.to(self.device), reduction=\"batchmean\", log_target=True).item()\n",
    "            #     running_kl *= (total_logprobs / (total_logprobs + target_log_probs.numel()))\n",
    "            #     running_kl += (target_log_probs.numel() / (total_logprobs + target_log_probs.numel())) * kl\n",
    "            #     total_logprobs += target_log_probs.numel()\n",
    "            #     del target_log_probs, logits, kl\n",
    "            #     torch.cuda.empty_cache()\n",
    "            # reward = -running_kl\n",
    "            reward = -eval_ppl(self.model, self.test_data, self.sequence_length, device=self.device)\n",
    "\n",
    "        return self.get_state(), reward, done, False, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01a6cc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FineWeb-Edu v2\n",
      "Total tokens loaded: 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mldfrancis\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/ephemeral/esie/notebooks/wandb/run-20251002_050550-rk4ybt0o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ldfrancis/RLPress/runs/rk4ybt0o' target=\"_blank\">fanciful-wind-213</a></strong> to <a href='https://wandb.ai/ldfrancis/RLPress' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ldfrancis/RLPress' target=\"_blank\">https://wandb.ai/ldfrancis/RLPress</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ldfrancis/RLPress/runs/rk4ybt0o' target=\"_blank\">https://wandb.ai/ldfrancis/RLPress/runs/rk4ybt0o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "class DebugEnvironment:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_layer = 0\n",
    "        self.n_layers = 5\n",
    "        self.actions = []\n",
    "        return self.get_state(), {}\n",
    "\n",
    "    def get_state(self):\n",
    "        state = {\n",
    "            \"state\": torch.tensor([self.cur_layer / self.n_layers]*3, dtype=torch.float32),\n",
    "            \"action_mask\": torch.ones(N, dtype=torch.float32)\n",
    "        }\n",
    "        # state = torch.tensor([self.cur_layer / self.n_layers]*3, dtype=torch.float32)\n",
    "        return state\n",
    "\n",
    "    def step(self, action:int):\n",
    "        self.actions.append(action)\n",
    "        self.cur_layer += 1\n",
    "        done = self.cur_layer == self.n_layers\n",
    "        reward = 0\n",
    "        if done:\n",
    "            target = [1,2,3,0,3]\n",
    "            diff = 0\n",
    "            for i in range(len(self.actions)):\n",
    "                diff += abs(self.actions[i] - target[i])\n",
    "            reward = max(0, 10 - diff)\n",
    "        \n",
    "        return self.get_state(), reward*100, done, False, {}\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_size:int, action_size:int, device:str=device):\n",
    "        super(Policy, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.device = device\n",
    "        self.base = nn.Sequential(\n",
    "            nn.Linear(state_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.head = nn.Linear(256, action_size)\n",
    "        self.uniform_init()\n",
    "\n",
    "    def to(self, device:Optional[str]=None):\n",
    "        if device is None:\n",
    "            device = self.device\n",
    "        self.device = device\n",
    "        return super().to(device)\n",
    "\n",
    "    def forward(self, state:Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        large_neg = torch.finfo(state.dtype).min\n",
    "        action_mask = state[:, -self.action_size:]\n",
    "\n",
    "        x = self.base(state)\n",
    "        logits = self.head(x)\n",
    "        logits = torch.where(action_mask.to(self.device) == 1, logits, large_neg)\n",
    "        \n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        return dist\n",
    "\n",
    "    def uniform_init(self):\n",
    "        bias = self.head.bias.data.detach().clone()\n",
    "        bias = torch.ones_like(bias)*(1/self.action_size)\n",
    "        self.head.bias.data.copy_(bias)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def act(self, state:Dict[str, torch.Tensor], deterministic=False) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        if len(state.shape) == 1:\n",
    "            state = state.unsqueeze(0)\n",
    "        dist = self(state)\n",
    "        action = dist.sample() if not deterministic else dist.mode\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action, log_prob\n",
    "\n",
    "\n",
    "class Value(nn.Module):\n",
    "    def __init__(self, state_size:int, device:str):\n",
    "        super(Value, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        self.device = device\n",
    "\n",
    "    def to(self, device:Optional[str]=None):\n",
    "        if device is None:\n",
    "            device = self.device\n",
    "        self.device = device\n",
    "        return super().to(device)\n",
    "\n",
    "    def forward(self, state:torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(state)\n",
    "\n",
    "\n",
    "class PolicyValue:\n",
    "    def __init__(self, policy_model: nn.Module, value_model: nn.Module):\n",
    "        self.policy_model = policy_model\n",
    "        self.value_model = value_model\n",
    "        self.device = policy_model.device\n",
    "\n",
    "    def to(self, device:Optional[str]=None):\n",
    "        if device is None:\n",
    "            device = self.device\n",
    "        self.device = device\n",
    "        self.policy_model.to(device)\n",
    "        self.value_model.to(device)\n",
    "        return self\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        dist = self.policy_model(x)\n",
    "        value = self.value_model(x)\n",
    "        if action is None:\n",
    "            action = dist.sample()\n",
    "        return action, dist.log_prob(action), dist.entropy(), value\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.value_model(x)\n",
    "    \n",
    "    def get_dist(self, x):\n",
    "        return self.policy_model(x)\n",
    "    \n",
    "\n",
    "\n",
    "def process_trajectory(trajectory, gamma, lam, device):\n",
    "    lastgaelam = 0\n",
    "    steps = len(trajectory)\n",
    "    advantages = torch.zeros(steps).to(device)\n",
    "\n",
    "    states, actions, rewards, log_probs, values = [], [], [], [], []\n",
    "    for trans in trajectory:\n",
    "        state, action, reward, log_prob, value = trans\n",
    "        states.append(torch.tensor(state).to(device))\n",
    "        actions.append(torch.tensor(action).to(device))\n",
    "        rewards.append(torch.tensor(reward).to(device))\n",
    "        log_probs.append(torch.tensor(log_prob).to(device))\n",
    "        values.append(torch.tensor(value).to(device))\n",
    "\n",
    "    values = torch.cat(values)\n",
    "    states = torch.stack(states, dim=0)\n",
    "    actions = torch.cat(actions)\n",
    "    log_probs = torch.cat(log_probs)\n",
    "    rewards = torch.stack(rewards)\n",
    "\n",
    "    for t in reversed(range(steps)):\n",
    "        if t == steps - 1:\n",
    "            nextnonterminal = 0.0\n",
    "            nextvalue = 0.0\n",
    "        else:\n",
    "            nextnonterminal = 1.0\n",
    "            nextvalue = values[t+1]\n",
    "        delta = rewards[t] + gamma * nextvalue * nextnonterminal - values[t]\n",
    "        advantages[t] = lastgaelam = delta + gamma * lam * nextnonterminal * lastgaelam\n",
    "        \n",
    "    returns = advantages + values.squeeze()\n",
    "\n",
    "    return states, actions, log_probs, values, returns, advantages\n",
    "\n",
    "\n",
    "def process_trajectories(trajectories, gamma, lam, device):\n",
    "    states, actions, log_probs, values, returns, advantages = [], [], [], [], [], []\n",
    "    for trajectory in trajectories:\n",
    "        s, a, lp, v, r, adv = process_trajectory(trajectory, gamma, lam, device)\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        log_probs.append(lp)\n",
    "        values.append(v)\n",
    "        returns.append(r)\n",
    "        advantages.append(adv)\n",
    "    states = torch.cat(states, dim=0)\n",
    "    actions = torch.cat(actions, dim=0)\n",
    "    log_probs = torch.cat(log_probs, dim=0)\n",
    "    values = torch.cat(values, dim=0)\n",
    "    returns = torch.cat(returns, dim=0)\n",
    "    advantages = torch.cat(advantages, dim=0)\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "    return states, actions, log_probs, values, returns, advantages\n",
    "\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self):\n",
    "        self.step = 0\n",
    "\n",
    "    def log(self, metrics:Dict[str, float], step:Optional[int]=None):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def term(self, *args):\n",
    "        print(*args)\n",
    "\n",
    "\n",
    "class WandBLogger(Logger):\n",
    "    def __init__(self, entity:str=\"ldfrancis\", project_name:str=\"RLPress\"):\n",
    "        super().__init__()\n",
    "        wandb.init(project=project_name, entity=entity)\n",
    "\n",
    "    def log(self, metrics:Dict[str, float], step:Optional[int]=None):\n",
    "        if step is None:\n",
    "            step = self.step\n",
    "            self.step += 1\n",
    "        wandb.log(metrics, step=step)\n",
    "\n",
    "\n",
    "class TerminalLogger(Logger):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def log(self, metrics:Dict[str, float], step:Optional[int]=None):\n",
    "        if step is None:\n",
    "            step = self.step\n",
    "            self.step += 1\n",
    "        print(f\"Step {step}:\")\n",
    "        for k, v in metrics.items():\n",
    "            print(f\"\\t{k} : {v}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "class RLLearner:\n",
    "    def __init__(self, policy_n_value:PolicyValue, gamma: float = 0.99, lam: float = 0.95, lr=1e-4, device: str = device):\n",
    "        self.policy_n_value = policy_n_value\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.device = device\n",
    "        self.policy_optimizer = torch.optim.Adam(self.policy_n_value.policy_model.parameters(), lr=lr)\n",
    "        self.value_optimizer = torch.optim.Adam(self.policy_n_value.value_model.parameters(), lr=1e-5)\n",
    "        self.global_step = 0\n",
    "\n",
    "    def __call__(self, trajectories, epochs:int=10):\n",
    "        states, actions, log_probs, values, returns, advantages = process_trajectories(trajectories, self.gamma, lam=0.95, device=self.device)\n",
    "        max_grad_norm = 0.5\n",
    "        target_kl = 0.01\n",
    "        self.global_step += len(states)\n",
    "       \n",
    "        bs = 32\n",
    "        inds = np.arange(0, len(states))\n",
    "        clip_coef = 0.2\n",
    "        clipfracs = []\n",
    "        \n",
    "        policy_losses = []\n",
    "        value_losses = []\n",
    "        entropy_losses = []\n",
    "        approx_kls = []\n",
    "        old_approx_kls = []\n",
    "        grad_steps = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            stop_updates = False\n",
    "            np.random.shuffle(inds)\n",
    "            # self.policy_optimizer.zero_grad(); self.value_optimizer.zero_grad()\n",
    "            for start in range(0, len(states), bs):\n",
    "                end = min(start+bs, len(states))\n",
    "                b_inds = inds[start:end]\n",
    "\n",
    "                x = states[b_inds].to(self.device)\n",
    "                a = actions[b_inds].to(self.device)\n",
    "\n",
    "                dist = self.policy_n_value.get_dist(x)\n",
    "                newlogprob = dist.log_prob(a)\n",
    "                entropy = dist.entropy()\n",
    "                newvalue = self.policy_n_value.get_value(x)\n",
    "                \n",
    "                logratio = newlogprob - log_probs[b_inds].to(self.device)\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                # Policy loss\n",
    "                pg_obj1 = advantages[b_inds] * ratio\n",
    "                pg_obj2 = advantages[b_inds] * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
    "                pg_loss = -torch.min(pg_obj1, pg_obj2).mean()\n",
    "\n",
    "                # Value loss\n",
    "                v_loss = 0.5 * ((newvalue - returns[b_inds])**2).mean()\n",
    "\n",
    "                # Entropy loss\n",
    "                entropy_loss = -entropy.mean()\n",
    "\n",
    "                # Combined loss\n",
    "                loss = pg_loss + 0.5 * entropy_loss\n",
    "\n",
    "                self.policy_optimizer.zero_grad(); self.value_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                v_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.policy_n_value.value_model.parameters(), max_grad_norm)\n",
    "                nn.utils.clip_grad_norm_(self.policy_n_value.policy_model.parameters(), max_grad_norm)\n",
    "                self.policy_optimizer.step(); self.value_optimizer.step()\n",
    "\n",
    "                # Approx kl\n",
    "                with torch.no_grad():\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = (ratio - 1 - logratio).mean()\n",
    "                    clipfracs += [((ratio > (1 + clip_coef)) | (ratio < (1 - clip_coef))).float().mean().item()]\n",
    "\n",
    "                grad_steps += 1\n",
    "                policy_losses += [pg_loss.item()]\n",
    "                value_losses += [v_loss.item()]\n",
    "                entropy_losses += [entropy_loss.item()]\n",
    "                approx_kls += [approx_kl.item()]\n",
    "                old_approx_kls += [old_approx_kl.item()]\n",
    "\n",
    "                # if approx_kl > target_kl:\n",
    "                #     stop_updates = True\n",
    "                #     break\n",
    "            # self.policy_optimizer.step(); self.value_optimizer.step()\n",
    "            if stop_updates:\n",
    "                break\n",
    "\n",
    "        learner_results = {\n",
    "            \"learner/losses/policy_loss\": np.mean(policy_losses) if policy_losses else 0,\n",
    "            \"learner/losses/value_loss\": np.mean(value_losses) if value_losses else 0,\n",
    "            \"learner/losses/entropy_loss\": np.mean(entropy_losses) if entropy_losses else 0,\n",
    "            \"learner/losses/approx_kls\": np.mean(approx_kls) if approx_kls else 0,\n",
    "            \"learner/losses/old_approx_kls\": np.mean(old_approx_kls) if old_approx_kls else 0,\n",
    "            \"learner/losses/clipfrac\": np.mean(clipfracs),\n",
    "            \"global_step\": self.global_step,\n",
    "        }\n",
    "\n",
    "        return learner_results\n",
    "        \n",
    "\n",
    "class PolicyValueRollout:\n",
    "    def __init__(self, env: DebugEnvironment, policy_n_value: PolicyValue):\n",
    "        self.env = env\n",
    "        self.policy_n_value = policy_n_value\n",
    "        self.policy_model = policy_n_value.policy_model\n",
    "        self.value_model = policy_n_value.value_model\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, deterministic=False):\n",
    "        state, _ = self.env.reset()\n",
    "        done = False\n",
    "        trajectory = []\n",
    "        step = 0\n",
    "        while not done:\n",
    "            # import pdb; pdb.set_trace()\n",
    "            state = torch.cat([state[\"state\"], state[\"action_mask\"]], dim=0).float().to(self.policy_n_value.device)\n",
    "            if not deterministic:\n",
    "                action, log_prob, _, value = self.policy_n_value.get_action_and_value(state.unsqueeze(0))\n",
    "            else:\n",
    "                action, log_prob = self.policy_model.act(state, deterministic=True)\n",
    "                value = self.value_model(state.unsqueeze(0))\n",
    "            next_state, reward, done, truncated, info = self.env.step(action.item())\n",
    "            done = done or truncated\n",
    "            trajectory.append((state, action, reward, log_prob, value))\n",
    "            state = next_state\n",
    "        return trajectory\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, env: DebugEnvironment, policy_n_value: PolicyValue, logger: Logger, gamma: float = 0.99, lam: float = 0.95, lr: float = 1e-4):\n",
    "        self.env = env\n",
    "        self.policy_n_value = policy_n_value\n",
    "        self.learner = RLLearner(policy_n_value, gamma=gamma, lam=lam, lr=lr, device=policy_n_value.device)\n",
    "        self.rollout = PolicyValueRollout(env, policy_n_value)\n",
    "        self.logger = logger\n",
    "        self.best_score = 0\n",
    "\n",
    "    def __call__(self, num_iters:int=100):\n",
    "        for iter in range(num_iters):\n",
    "            start_time = time.time()\n",
    "            trajectories = [self.rollout() for _ in range(4)]\n",
    "            learner_results = self.learner(trajectories)\n",
    "            with torch.no_grad():\n",
    "                trj =  self.rollout(deterministic=True)\n",
    "                rew = sum(tran[2] for tran in trj)\n",
    "                if rew > self.best_score:\n",
    "                    self.best_score = rew\n",
    "                    torch.save(self.policy_n_value.policy_model.state_dict(), \"best_policy.pt\")\n",
    "                    print(f\"New best model saved with score {self.best_score}\")\n",
    "            end_time = time.time()\n",
    "            loss = learner_results[\"learner/losses/policy_loss\"]\n",
    "            self.logger.log({**learner_results, \"Score\": rew}, step=learner_results[\"global_step\"])\n",
    "            print(f\"Iteration {iter+1}/{num_iters}, Loss: {loss:.4f}, Rew: {rew:.2f}, Global Step: {learner_results['global_step']}, Time: {end_time - start_time:.2f}s\")\n",
    "            del trajectories, trj\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "N = 10\n",
    "S = 3\n",
    "# env = gym.make(\"CartPole-v1\", max_episode_steps=200) #DebugEnvironment()\n",
    "# env = DebugEnvironment()\n",
    "env = Environment(model_name=\"meta-llama/Llama-2-7b-hf\", num_samples=32, sequence_length=2048, target_sparsity=0.5)\n",
    "\n",
    "policy_model = Policy(state_size=S+N, action_size=N, device=device)\n",
    "value_model = Value(state_size=S+N, device=device)\n",
    "\n",
    "policy_n_value = PolicyValue(policy_model, value_model)\n",
    "policy_n_value.to(device)\n",
    "\n",
    "logger = WandBLogger(entity=\"ldfrancis\", project_name=\"RLPress\")\n",
    "trainer = Trainer(env, policy_n_value, logger=logger, gamma=1.0, lam=0.95, lr=1e-3)\n",
    "# trainer(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bed356a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "Achieved Score: -5.540319442749023\n"
     ]
    }
   ],
   "source": [
    "policy_model.load_state_dict(torch.load(\"best_policy.pt\"))\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "score = 0\n",
    "while not done:\n",
    "    state = torch.cat([state[\"state\"], state[\"action_mask\"]], dim=0).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        dist = policy_model(state.unsqueeze(0))\n",
    "        action = dist.mode\n",
    "    next_state, reward, done, truncated, info = env.step(action.item())\n",
    "    done = done or truncated\n",
    "    state = next_state\n",
    "    score += reward\n",
    "    print(action.item())\n",
    "print(f\"Achieved Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c286a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.1937019782886529)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([m.weight.data.eq(0).float().mean().item() for layer in env.layers for m in layer.modules() if isinstance(m, nn.Linear)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f2b86e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
