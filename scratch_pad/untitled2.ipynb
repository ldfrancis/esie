{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87388c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abef8bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a276ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.conda/envs/esie/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Dict, Tuple\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import os\n",
    "try:\n",
    "    import wandb\n",
    "except ImportError:\n",
    "    print(\"wandb not installed. WandBLogger will not work.\")\n",
    "    wandb = None\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5debb54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Environment:\n",
    "    def __init__(self, model_name:str, num_samples:int, sequence_length:int, target_sparsity:float=0.5)->None:\n",
    "        self.model_name = model_name\n",
    "        self.num_samples = num_samples\n",
    "        self.sequence_length = sequence_length\n",
    "        self.target_sparsity = target_sparsity\n",
    "        self.num_samples = num_samples\n",
    "        self.sequence_length = sequence_length\n",
    "        self.possible_sparsities = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "        self.device = device if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # initialize state\n",
    "        self.load_calibration_data()\n",
    "        self.reset()\n",
    "\n",
    "    def load_calibration_data(self):\n",
    "        # caliberation data\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        self.tokenizer = tokenizer\n",
    "        num_tokens = self.num_samples * self.sequence_length\n",
    "        self.calib_data = get_fineweb_edu(num_tokens, self.sequence_length, tokenizer, train=True)\n",
    "        # self.test_data = get_fineweb_edu(num_tokens, self.sequence_length, tokenizer, train=False)\n",
    "        _, self.test_data = get_w2_data(self.num_samples, self.sequence_length, tokenizer)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def init(self) -> None:\n",
    "        # create model, tokenizer, and calibration data.\n",
    "        # model and tokenizer\n",
    "        model = AutoModelForCausalLM.from_pretrained(self.model_name, dtype=torch.float16, device_map=\"auto\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # caliberation data\n",
    "        test_data = self.test_data\n",
    "\n",
    "        # env attributes\n",
    "        self.action_mask = torch.ones(N)\n",
    "        self.layers = model.model.layers\n",
    "        self.num_layers = len(self.layers)\n",
    "        self.current_layer = 0\n",
    "        self.global_sparsity = 0.0\n",
    "        self.layer_sparsities = [0.0] * self.num_layers\n",
    "        self.pruning_info = {}\n",
    "\n",
    "        # buffers\n",
    "        self.inps = torch.zeros((self.num_samples, self.sequence_length, model.config.hidden_size), dtype=torch.float16, device=self.device)\n",
    "        self.outs = torch.zeros_like(self.inps)\n",
    "        self.inp_kwargs = {}\n",
    "\n",
    "        # obtain input into the first decoder layer\n",
    "        cache = model.config.use_cache\n",
    "        model.config.use_cache = False\n",
    "        inps = self.inps\n",
    "        inp_kwargs = self.inp_kwargs\n",
    "        class catch_inps(nn.Module):\n",
    "            def __init__(self, module):\n",
    "                super().__init__()\n",
    "                self.module = module\n",
    "                self.num_inps = 0\n",
    "            def forward(self, inp, **kwargs):\n",
    "                nonlocal inps, inp_kwargs\n",
    "                inps[self.num_inps] = inp\n",
    "                inp_kwargs.update(kwargs)\n",
    "                self.num_inps += 1\n",
    "                raise Exception(\"caught inps. Stopping forward pass.\")\n",
    "        self.layers[0] = catch_inps(self.layers[0])\n",
    "        for sample in self.calib_data:\n",
    "            try:\n",
    "                model(sample.to(self.device))\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        self.layers[0] = self.layers[0].module\n",
    "        self.inps = inps\n",
    "        self.inp_kwargs = inp_kwargs\n",
    "\n",
    "        # save the log targets to a file for computing the KL divergence later\n",
    "        i_batches = 0\n",
    "        os.makedirs(f\"logs/kl/{self.model_name}\", exist_ok=True)\n",
    "        batch_size = 4\n",
    "        log_probs = []\n",
    "        for j in range(self.num_samples):\n",
    "            if os.path.exists(f\"logs/kl/{self.model_name}/log_targets_{(j//batch_size)}_{batch_size}.pt\"):\n",
    "                i_batches = j // batch_size\n",
    "                continue\n",
    "            sample = test_data[j]\n",
    "            logits = model(sample.to(self.device)).logits\n",
    "            log_probs.append(F.log_softmax(logits.float(), dim=-1).reshape(-1, model.config.vocab_size).cpu())\n",
    "            if j % batch_size == batch_size-1:\n",
    "                log_probs = torch.cat(log_probs, dim=0).cpu()\n",
    "                torch.save(log_probs, f\"logs/kl/{self.model_name}/log_targets_{i_batches}_{batch_size}.pt\")\n",
    "                print(f\"Saved logs/kl/{self.model_name}/log_targets_{i_batches}_{batch_size}.pt\")\n",
    "                log_probs = []\n",
    "            elif j == self.num_samples - 1 and len(log_probs) > 0:\n",
    "                log_probs = torch.cat(log_probs, dim=0).cpu()\n",
    "                torch.save(log_probs, f\"logs/kl/{self.model_name}/log_targets_{i_batches}_{batch_size}.pt\")\n",
    "                print(f\"Saved logs/kl/{self.model_name}/log_targets_{i_batches}_{batch_size}.pt\")\n",
    "            i_batches = j // batch_size\n",
    "            \n",
    "        # create a dataloader for computing KL divergence later\n",
    "        model_name = self.model_name\n",
    "        class KLDataset(torch.utils.data.Dataset):\n",
    "            def __init__(self):\n",
    "                self.path_format = f\"logs/kl/{model_name}\"+\"/log_targets_{}_{}.pt\"\n",
    "            def __len__(self):\n",
    "                return i_batches + 1\n",
    "            def __getitem__(self, idx):\n",
    "                nonlocal batch_size\n",
    "                samples = torch.cat(test_data[idx*batch_size:(idx+1)*batch_size], dim=0)\n",
    "                log_probs = torch.load(self.path_format.format(idx, batch_size))\n",
    "                return samples, log_probs\n",
    "        self.kl_dataloader = torch.utils.data.DataLoader(KLDataset(), batch_size=1, shuffle=False)\n",
    "        # print(f\"KL dataloader with {len(self.kl_dataloader)} batches created.\")\n",
    "        model.config.use_cache = cache\n",
    "\n",
    "    def prune_layer(self, layer_idx:int, sparsity:float)->None:\n",
    "        if layer_idx in self.pruning_info:\n",
    "            raise Exception(f\"Layer {layer_idx} already pruned. Skipping.\")\n",
    "        \n",
    "        layer = self.layers[layer_idx]\n",
    "        sublayers = {name: module for name, module in layer.named_modules() if isinstance(module, nn.Linear)}\n",
    "        wrapped_layers = {}\n",
    "        for name, sublayer in sublayers.items():\n",
    "            wrapped_layers[name] = WrappedGPT(sublayer)\n",
    "\n",
    "        # obtain the input activations to each sublayer, computing the feature-wise norms\n",
    "        def add_batch(name):\n",
    "            def tmp(_, inp, out):\n",
    "                wrapped_layers[name].add_batch(inp[0].data, out.data)\n",
    "            return tmp\n",
    "        handles = []\n",
    "        for name in wrapped_layers:\n",
    "            handles.append(sublayers[name].register_forward_hook(add_batch(name)))\n",
    "        for j in range(self.num_samples):\n",
    "            self.outs[j] = layer(self.inps[j].unsqueeze(0), **self.inp_kwargs)[0]\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "        \n",
    "        for name in sublayers:\n",
    "            wrapped_layers[name].prune(sparsity)\n",
    "            wrapped_layers[name].clean()\n",
    "\n",
    "        # outputs after pruning\n",
    "        for j in range(self.num_samples):\n",
    "            with torch.no_grad():\n",
    "                self.outs[j] = layer(self.inps[j].unsqueeze(0), **self.inp_kwargs)[0]\n",
    "\n",
    "        # the output from this layer should be the input to the next layer\n",
    "        self.inps, self.outs = self.outs, self.inps\n",
    "\n",
    "        # done pruning this layer. Prepare some info about this layer's pruning\n",
    "        obtained_sparsity = np.mean([l.weight.data.eq(0).float().mean().item() for l in sublayers.values()]).item()\n",
    "        info = {\n",
    "            \"layer\": layer_idx,\n",
    "            \"layer_target_sparsity\": sparsity,\n",
    "            \"layer_obtained_sparsity\": obtained_sparsity,\n",
    "        }\n",
    "        self.pruning_info[layer_idx] = info\n",
    "\n",
    "    def reset(self) -> Dict[str, torch.Tensor]:\n",
    "        if hasattr(self, \"inps\"):\n",
    "            del self.inps, self.outs, self.inp_kwargs\n",
    "            del self.kl_dataloader\n",
    "            del self.model, self.tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "        self.init()\n",
    "        return self.get_state(), {}\n",
    "\n",
    "    def get_state(self) -> Dict[str, torch.Tensor]:\n",
    "        s = [self.global_sparsity, self.target_sparsity, self.current_layer / self.num_layers]\n",
    "        if self.current_layer == 0:\n",
    "            mask = [1] * len(self.possible_sparsities)\n",
    "        else:\n",
    "            mask = [1 if (sum(self.layer_sparsities[:self.current_layer]) + s) / self.current_layer <= self.target_sparsity else 0 for s in self.possible_sparsities]\n",
    "        state = {\n",
    "            \"state\": torch.tensor(s, dtype=torch.float32),\n",
    "            \"action_mask\": torch.tensor(mask, dtype=torch.float32)\n",
    "        }\n",
    "        return state\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, action:int)->Tuple[Dict[str, torch.Tensor], float, bool, Dict[str, object]]:\n",
    "        sparsity = self.possible_sparsities[action]\n",
    "        self.prune_layer(self.current_layer, sparsity)\n",
    "        # update global sparsity\n",
    "        self.layer_sparsities[self.current_layer] = sparsity\n",
    "        self.current_layer += 1\n",
    "        self.global_sparsity = np.mean(self.layer_sparsities[:self.current_layer])\n",
    "        # compute reward\n",
    "        reward = 0\n",
    "        done = self.current_layer == self.num_layers\n",
    "        if done:\n",
    "            # compute KL divergence between the pruned and unpruned model.\n",
    "            # the logits have been saved to a file during initialization.\n",
    "            running_kl = 0.0\n",
    "            total_logprobs = 0\n",
    "            # for batch in self.kl_dataloader:\n",
    "            #     inps, target_log_probs = [batch[0].squeeze(0), batch[1].squeeze(0)]\n",
    "            #     logits = self.model(inps.to(self.device)).logits.reshape(-1, self.model.config.vocab_size)\n",
    "            #     log_probs = F.log_softmax(logits.float(), dim=-1)\n",
    "            #     kl = F.kl_div(log_probs, target_log_probs.to(self.device), reduction=\"batchmean\", log_target=True).item()\n",
    "            #     running_kl *= (total_logprobs / (total_logprobs + target_log_probs.numel()))\n",
    "            #     running_kl += (target_log_probs.numel() / (total_logprobs + target_log_probs.numel())) * kl\n",
    "            #     total_logprobs += target_log_probs.numel()\n",
    "            #     del target_log_probs, logits, kl\n",
    "            #     torch.cuda.empty_cache()\n",
    "            # reward = -running_kl\n",
    "            reward = -eval_ppl(self.model, self.test_data, self.sequence_length, device=self.device)\n",
    "\n",
    "        return self.get_state(), reward, done, False, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01a6cc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FineWeb-Edu v2\n",
      "Total tokens loaded: 65536\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 387\u001b[0m\n\u001b[1;32m    384\u001b[0m S \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;66;03m# env = gym.make(\"CartPole-v1\", max_episode_steps=200) #DebugEnvironment()\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;66;03m# env = DebugEnvironment()\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mEnvironment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Llama-2-7b-hf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_sparsity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m policy_model \u001b[38;5;241m=\u001b[39m Policy(state_size\u001b[38;5;241m=\u001b[39mS\u001b[38;5;241m+\u001b[39mN, action_size\u001b[38;5;241m=\u001b[39mN, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    390\u001b[0m value_model \u001b[38;5;241m=\u001b[39m Value(state_size\u001b[38;5;241m=\u001b[39mS\u001b[38;5;241m+\u001b[39mN, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m, in \u001b[0;36mEnvironment.__init__\u001b[0;34m(self, model_name, num_samples, sequence_length, target_sparsity)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# initialize state\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_calibration_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n",
      "Cell \u001b[0;32mIn[4], line 25\u001b[0m, in \u001b[0;36mEnvironment.load_calibration_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalib_data \u001b[38;5;241m=\u001b[39m get_fineweb_edu(num_tokens, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_length, tokenizer, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# self.test_data = get_fineweb_edu(num_tokens, self.sequence_length, tokenizer, train=False)\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m _, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_w2_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/esie/notebooks/utils.py:25\u001b[0m, in \u001b[0;36mget_w2_data\u001b[0;34m(num_samples, seq_len, tokenizer)\u001b[0m\n\u001b[1;32m     22\u001b[0m w2_train_dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwikitext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwikitext-2-raw-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m w2_val_dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwikitext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwikitext-2-raw-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m train_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw2_train_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[1;32m     26\u001b[0m test_tokens \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(w2_val_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]), return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[1;32m     28\u001b[0m num_tokens \u001b[38;5;241m=\u001b[39m train_tokens\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/esie/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2938\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2936\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2937\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2938\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2940\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/.conda/envs/esie/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3048\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3026\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   3027\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   3028\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3045\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3046\u001b[0m     )\n\u001b[1;32m   3047\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3048\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3051\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3057\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3058\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3059\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3060\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3061\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3062\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3063\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3064\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3065\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3067\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3068\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/esie/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3123\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3094\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3095\u001b[0m \u001b[38;5;124;03mTokenize and prepare for the model a sequence or a pair of sequences.\u001b[39;00m\n\u001b[1;32m   3096\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3111\u001b[0m \u001b[38;5;124;03m        method).\u001b[39;00m\n\u001b[1;32m   3112\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3114\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3115\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3116\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3120\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3121\u001b[0m )\n\u001b[0;32m-> 3123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3126\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3142\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msplit_special_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3143\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3144\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/esie/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:627\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    605\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    625\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    626\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 627\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m~/.conda/envs/esie/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:553\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m!=\u001b[39m split_special_tokens:\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m=\u001b[39m split_special_tokens\n\u001b[0;32m--> 553\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: tuple[\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m#                       list[dict[str, list[list[int]]]] or list[dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m#                       list[EncodingFast]\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    565\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    567\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    577\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "class DebugEnvironment:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_layer = 0\n",
    "        self.n_layers = 5\n",
    "        self.actions = []\n",
    "        return self.get_state(), {}\n",
    "\n",
    "    def get_state(self):\n",
    "        state = {\n",
    "            \"state\": torch.tensor([self.cur_layer / self.n_layers]*3, dtype=torch.float32),\n",
    "            \"action_mask\": torch.ones(N, dtype=torch.float32)\n",
    "        }\n",
    "        # state = torch.tensor([self.cur_layer / self.n_layers]*3, dtype=torch.float32)\n",
    "        return state\n",
    "\n",
    "    def step(self, action:int):\n",
    "        self.actions.append(action)\n",
    "        self.cur_layer += 1\n",
    "        done = self.cur_layer == self.n_layers\n",
    "        reward = 0\n",
    "        if done:\n",
    "            target = [1,2,3,0,3]\n",
    "            diff = 0\n",
    "            for i in range(len(self.actions)):\n",
    "                diff += abs(self.actions[i] - target[i])\n",
    "            reward = max(0, 10 - diff)\n",
    "        \n",
    "        return self.get_state(), reward*100, done, False, {}\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_size:int, action_size:int, device:str=device):\n",
    "        super(Policy, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.device = device\n",
    "        self.base = nn.Sequential(\n",
    "            nn.Linear(state_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.head = nn.Linear(256, action_size)\n",
    "        self.uniform_init()\n",
    "\n",
    "    def to(self, device:Optional[str]=None):\n",
    "        if device is None:\n",
    "            device = self.device\n",
    "        self.device = device\n",
    "        return super().to(device)\n",
    "\n",
    "    def forward(self, state:Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        large_neg = torch.finfo(state.dtype).min\n",
    "        action_mask = state[:, -self.action_size:]\n",
    "\n",
    "        x = self.base(state)\n",
    "        logits = self.head(x)\n",
    "        logits = torch.where(action_mask.to(self.device) == 1, logits, large_neg)\n",
    "        \n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        return dist\n",
    "\n",
    "    def uniform_init(self):\n",
    "        bias = self.head.bias.data.detach().clone()\n",
    "        bias = torch.ones_like(bias)*(1/self.action_size)\n",
    "        self.head.bias.data.copy_(bias)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def act(self, state:Dict[str, torch.Tensor], deterministic=False) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        if len(state.shape) == 1:\n",
    "            state = state.unsqueeze(0)\n",
    "        dist = self(state)\n",
    "        action = dist.sample() if not deterministic else dist.mode\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action, log_prob\n",
    "\n",
    "\n",
    "class Value(nn.Module):\n",
    "    def __init__(self, state_size:int, device:str):\n",
    "        super(Value, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        self.device = device\n",
    "\n",
    "    def to(self, device:Optional[str]=None):\n",
    "        if device is None:\n",
    "            device = self.device\n",
    "        self.device = device\n",
    "        return super().to(device)\n",
    "\n",
    "    def forward(self, state:torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(state)\n",
    "\n",
    "\n",
    "class PolicyValue:\n",
    "    def __init__(self, policy_model: nn.Module, value_model: nn.Module):\n",
    "        self.policy_model = policy_model\n",
    "        self.value_model = value_model\n",
    "        self.device = policy_model.device\n",
    "\n",
    "    def to(self, device:Optional[str]=None):\n",
    "        if device is None:\n",
    "            device = self.device\n",
    "        self.device = device\n",
    "        self.policy_model.to(device)\n",
    "        self.value_model.to(device)\n",
    "        return self\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        dist = self.policy_model(x)\n",
    "        value = self.value_model(x)\n",
    "        if action is None:\n",
    "            action = dist.sample()\n",
    "        return action, dist.log_prob(action), dist.entropy(), value\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.value_model(x)\n",
    "    \n",
    "    def get_dist(self, x):\n",
    "        return self.policy_model(x)\n",
    "    \n",
    "\n",
    "\n",
    "def process_trajectory(trajectory, gamma, lam, device):\n",
    "    lastgaelam = 0\n",
    "    steps = len(trajectory)\n",
    "    advantages = torch.zeros(steps).to(device)\n",
    "\n",
    "    states, actions, rewards, log_probs, values = [], [], [], [], []\n",
    "    for trans in trajectory:\n",
    "        state, action, reward, log_prob, value = trans\n",
    "        states.append(torch.tensor(state).to(device))\n",
    "        actions.append(torch.tensor(action).to(device))\n",
    "        rewards.append(torch.tensor(reward).to(device))\n",
    "        log_probs.append(torch.tensor(log_prob).to(device))\n",
    "        values.append(torch.tensor(value).to(device))\n",
    "\n",
    "    values = torch.cat(values)\n",
    "    states = torch.stack(states, dim=0)\n",
    "    actions = torch.cat(actions)\n",
    "    log_probs = torch.cat(log_probs)\n",
    "    rewards = torch.stack(rewards)\n",
    "\n",
    "    for t in reversed(range(steps)):\n",
    "        if t == steps - 1:\n",
    "            nextnonterminal = 0.0\n",
    "            nextvalue = 0.0\n",
    "        else:\n",
    "            nextnonterminal = 1.0\n",
    "            nextvalue = values[t+1]\n",
    "        delta = rewards[t] + gamma * nextvalue * nextnonterminal - values[t]\n",
    "        advantages[t] = lastgaelam = delta + gamma * lam * nextnonterminal * lastgaelam\n",
    "        \n",
    "    returns = advantages + values.squeeze()\n",
    "\n",
    "    return states, actions, log_probs, values, returns, advantages\n",
    "\n",
    "\n",
    "def process_trajectories(trajectories, gamma, lam, device):\n",
    "    states, actions, log_probs, values, returns, advantages = [], [], [], [], [], []\n",
    "    for trajectory in trajectories:\n",
    "        s, a, lp, v, r, adv = process_trajectory(trajectory, gamma, lam, device)\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        log_probs.append(lp)\n",
    "        values.append(v)\n",
    "        returns.append(r)\n",
    "        advantages.append(adv)\n",
    "    states = torch.cat(states, dim=0)\n",
    "    actions = torch.cat(actions, dim=0)\n",
    "    log_probs = torch.cat(log_probs, dim=0)\n",
    "    values = torch.cat(values, dim=0)\n",
    "    returns = torch.cat(returns, dim=0)\n",
    "    advantages = torch.cat(advantages, dim=0)\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "    return states, actions, log_probs, values, returns, advantages\n",
    "\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self):\n",
    "        self.step = 0\n",
    "\n",
    "    def log(self, metrics:Dict[str, float], step:Optional[int]=None):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def term(self, *args):\n",
    "        print(*args)\n",
    "\n",
    "\n",
    "class WandBLogger(Logger):\n",
    "    def __init__(self, entity:str=\"ldfrancis\", project_name:str=\"RLPress\"):\n",
    "        super().__init__()\n",
    "        wandb.init(project=project_name, entity=entity)\n",
    "\n",
    "    def log(self, metrics:Dict[str, float], step:Optional[int]=None):\n",
    "        if step is None:\n",
    "            step = self.step\n",
    "            self.step += 1\n",
    "        wandb.log(metrics, step=step)\n",
    "\n",
    "\n",
    "class TerminalLogger(Logger):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def log(self, metrics:Dict[str, float], step:Optional[int]=None):\n",
    "        if step is None:\n",
    "            step = self.step\n",
    "            self.step += 1\n",
    "        print(f\"Step {step}:\")\n",
    "        for k, v in metrics.items():\n",
    "            print(f\"\\t{k} : {v}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "class RLLearner:\n",
    "    def __init__(self, policy_n_value:PolicyValue, gamma: float = 0.99, lam: float = 0.95, lr=1e-4, device: str = device):\n",
    "        self.policy_n_value = policy_n_value\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.device = device\n",
    "        self.policy_optimizer = torch.optim.Adam(self.policy_n_value.policy_model.parameters(), lr=lr)\n",
    "        self.value_optimizer = torch.optim.Adam(self.policy_n_value.value_model.parameters(), lr=1e-5)\n",
    "        self.global_step = 0\n",
    "\n",
    "    def __call__(self, trajectories, epochs:int=10):\n",
    "        states, actions, log_probs, values, returns, advantages = process_trajectories(trajectories, self.gamma, lam=0.95, device=self.device)\n",
    "        max_grad_norm = 0.5\n",
    "        target_kl = 0.01\n",
    "        self.global_step += len(states)\n",
    "       \n",
    "        bs = 32\n",
    "        inds = np.arange(0, len(states))\n",
    "        clip_coef = 0.2\n",
    "        clipfracs = []\n",
    "        \n",
    "        policy_losses = []\n",
    "        value_losses = []\n",
    "        entropy_losses = []\n",
    "        approx_kls = []\n",
    "        old_approx_kls = []\n",
    "        grad_steps = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            stop_updates = False\n",
    "            np.random.shuffle(inds)\n",
    "            # self.policy_optimizer.zero_grad(); self.value_optimizer.zero_grad()\n",
    "            for start in range(0, len(states), bs):\n",
    "                end = min(start+bs, len(states))\n",
    "                b_inds = inds[start:end]\n",
    "\n",
    "                x = states[b_inds].to(self.device)\n",
    "                a = actions[b_inds].to(self.device)\n",
    "\n",
    "                dist = self.policy_n_value.get_dist(x)\n",
    "                newlogprob = dist.log_prob(a)\n",
    "                entropy = dist.entropy()\n",
    "                newvalue = self.policy_n_value.get_value(x)\n",
    "                \n",
    "                logratio = newlogprob - log_probs[b_inds].to(self.device)\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                # Policy loss\n",
    "                pg_obj1 = advantages[b_inds] * ratio\n",
    "                pg_obj2 = advantages[b_inds] * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
    "                pg_loss = -torch.min(pg_obj1, pg_obj2).mean()\n",
    "\n",
    "                # Value loss\n",
    "                v_loss = 0.5 * ((newvalue - returns[b_inds])**2).mean()\n",
    "\n",
    "                # Entropy loss\n",
    "                entropy_loss = -entropy.mean()\n",
    "\n",
    "                # Combined loss\n",
    "                loss = pg_loss + 0.5 * entropy_loss\n",
    "\n",
    "                self.policy_optimizer.zero_grad(); self.value_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                v_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.policy_n_value.value_model.parameters(), max_grad_norm)\n",
    "                nn.utils.clip_grad_norm_(self.policy_n_value.policy_model.parameters(), max_grad_norm)\n",
    "                self.policy_optimizer.step(); self.value_optimizer.step()\n",
    "\n",
    "                # Approx kl\n",
    "                with torch.no_grad():\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = (ratio - 1 - logratio).mean()\n",
    "                    clipfracs += [((ratio > (1 + clip_coef)) | (ratio < (1 - clip_coef))).float().mean().item()]\n",
    "\n",
    "                grad_steps += 1\n",
    "                policy_losses += [pg_loss.item()]\n",
    "                value_losses += [v_loss.item()]\n",
    "                entropy_losses += [entropy_loss.item()]\n",
    "                approx_kls += [approx_kl.item()]\n",
    "                old_approx_kls += [old_approx_kl.item()]\n",
    "\n",
    "                # if approx_kl > target_kl:\n",
    "                #     stop_updates = True\n",
    "                #     break\n",
    "            # self.policy_optimizer.step(); self.value_optimizer.step()\n",
    "            if stop_updates:\n",
    "                break\n",
    "\n",
    "        learner_results = {\n",
    "            \"learner/losses/policy_loss\": np.mean(policy_losses) if policy_losses else 0,\n",
    "            \"learner/losses/value_loss\": np.mean(value_losses) if value_losses else 0,\n",
    "            \"learner/losses/entropy_loss\": np.mean(entropy_losses) if entropy_losses else 0,\n",
    "            \"learner/losses/approx_kls\": np.mean(approx_kls) if approx_kls else 0,\n",
    "            \"learner/losses/old_approx_kls\": np.mean(old_approx_kls) if old_approx_kls else 0,\n",
    "            \"learner/losses/clipfrac\": np.mean(clipfracs),\n",
    "            \"global_step\": self.global_step,\n",
    "        }\n",
    "\n",
    "        return learner_results\n",
    "        \n",
    "\n",
    "class PolicyValueRollout:\n",
    "    def __init__(self, env: DebugEnvironment, policy_n_value: PolicyValue):\n",
    "        self.env = env\n",
    "        self.policy_n_value = policy_n_value\n",
    "        self.policy_model = policy_n_value.policy_model\n",
    "        self.value_model = policy_n_value.value_model\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, deterministic=False):\n",
    "        state, _ = self.env.reset()\n",
    "        done = False\n",
    "        trajectory = []\n",
    "        step = 0\n",
    "        while not done:\n",
    "            # import pdb; pdb.set_trace()\n",
    "            state = torch.cat([state[\"state\"], state[\"action_mask\"]], dim=0).float().to(self.policy_n_value.device)\n",
    "            if not deterministic:\n",
    "                action, log_prob, _, value = self.policy_n_value.get_action_and_value(state.unsqueeze(0))\n",
    "            else:\n",
    "                action, log_prob = self.policy_model.act(state, deterministic=True)\n",
    "                value = self.value_model(state.unsqueeze(0))\n",
    "            next_state, reward, done, truncated, info = self.env.step(action.item())\n",
    "            done = done or truncated\n",
    "            trajectory.append((state, action, reward, log_prob, value))\n",
    "            state = next_state\n",
    "        return trajectory\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, env: DebugEnvironment, policy_n_value: PolicyValue, logger: Logger, gamma: float = 0.99, lam: float = 0.95, lr: float = 1e-4):\n",
    "        self.env = env\n",
    "        self.policy_n_value = policy_n_value\n",
    "        self.learner = RLLearner(policy_n_value, gamma=gamma, lam=lam, lr=lr, device=policy_n_value.device)\n",
    "        self.rollout = PolicyValueRollout(env, policy_n_value)\n",
    "        self.logger = logger\n",
    "        self.best_score = -1e20\n",
    "\n",
    "    def __call__(self, num_iters:int=100):\n",
    "        for iter in range(num_iters):\n",
    "            start_time = time.time()\n",
    "            trajectories = [self.rollout() for _ in range(4)]\n",
    "            learner_results = self.learner(trajectories)\n",
    "            with torch.no_grad():\n",
    "                trj =  self.rollout(deterministic=True)\n",
    "                rew = sum(tran[2] for tran in trj)\n",
    "                if rew > self.best_score:\n",
    "                    self.best_score = rew\n",
    "                    torch.save(self.policy_n_value.policy_model.state_dict(), \"best_policy.pt\")\n",
    "                    print(f\"New best model saved with score {self.best_score}\")\n",
    "            end_time = time.time()\n",
    "            loss = learner_results[\"learner/losses/policy_loss\"]\n",
    "            self.logger.log({**learner_results, \"Score\": rew}, step=learner_results[\"global_step\"])\n",
    "            print(f\"Iteration {iter+1}/{num_iters}, Loss: {loss:.4f}, Rew: {rew:.2f}, Global Step: {learner_results['global_step']}, Time: {end_time - start_time:.2f}s\")\n",
    "            del trajectories, trj\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "N = 10\n",
    "S = 3\n",
    "# env = gym.make(\"CartPole-v1\", max_episode_steps=200) #DebugEnvironment()\n",
    "# env = DebugEnvironment()\n",
    "env = Environment(model_name=\"meta-llama/Llama-2-7b-hf\", num_samples=32, sequence_length=2048, target_sparsity=0.5)\n",
    "\n",
    "policy_model = Policy(state_size=S+N, action_size=N, device=device)\n",
    "value_model = Value(state_size=S+N, device=device)\n",
    "\n",
    "policy_n_value = PolicyValue(policy_model, value_model)\n",
    "policy_n_value.to(device)\n",
    "\n",
    "logger = WandBLogger(entity=\"ldfrancis\", project_name=\"RLPress\")\n",
    "trainer = Trainer(env, policy_n_value, logger=logger, gamma=1.0, lam=0.95, lr=1e-3)\n",
    "trainer(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed356a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'best_policy.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m policy_model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest_policy.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      2\u001b[0m state, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      3\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/esie/lib/python3.10/site-packages/torch/serialization.py:1484\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1482\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1484\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1485\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1486\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1488\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1489\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.conda/envs/esie/lib/python3.10/site-packages/torch/serialization.py:759\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 759\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    761\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.conda/envs/esie/lib/python3.10/site-packages/torch/serialization.py:740\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'best_policy.pt'"
     ]
    }
   ],
   "source": [
    "# policy_model.load_state_dict(torch.load(\"best_policy.pt\"))\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "score = 0\n",
    "while not done:\n",
    "    state = torch.cat([state[\"state\"], state[\"action_mask\"]], dim=0).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        dist = policy_model(state.unsqueeze(0))\n",
    "        action = dist.mode\n",
    "    next_state, reward, done, truncated, info = env.step(action.item())\n",
    "    done = done or truncated\n",
    "    state = next_state\n",
    "    score += reward\n",
    "    print(action.item())\n",
    "print(f\"Achieved Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c286a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.1937019782886529)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([m.weight.data.eq(0).float().mean().item() for layer in env.layers for m in layer.modules() if isinstance(m, nn.Linear)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7f2b86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FineWeb-Edu v2\n",
      "Total tokens loaded: 131072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]\n"
     ]
    }
   ],
   "source": [
    "env = Environment(model_name=\"meta-llama/Llama-2-7b-hf\", num_samples=32, sequence_length=4096, target_sparsity=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b49d203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "num_samples = 128\n",
    "sequence_length = 4096\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, dtype=torch.float16, device_map=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "_, test_data = get_w2_data(num_samples, sequence_length, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a8d435f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.116021633148193"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_ppl(env.model, test_data, sequence_length, device=env.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02e6432d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"dtype\": \"float16\",\n",
       "  \"eos_token_id\": 2,\n",
       "  \"head_dim\": 128,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 11008,\n",
       "  \"max_position_embeddings\": 4096,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"transformers_version\": \"4.57.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create compressed weights for each transformer block\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2179f9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FineWeb-Edu v2\n",
      "Total tokens loaded: 524288\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "prune_default() got an unexpected keyword argument 'save_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m num_tokens \u001b[38;5;241m=\u001b[39m num_samples \u001b[38;5;241m*\u001b[39m sequence_length\n\u001b[1;32m      6\u001b[0m calib_data \u001b[38;5;241m=\u001b[39m get_fineweb_edu(num_tokens, sequence_length, tokenizer, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mprune_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalib_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparsity_ratios\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.51\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta3\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.38\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_sparsegpt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/esie/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: prune_default() got an unexpected keyword argument 'save_dir'"
     ]
    }
   ],
   "source": [
    "save_dir = f\"logs/sparse_weights/{model_name.split('/')[-1]}\"\n",
    "num_samples = 128\n",
    "sequence_length = 4096\n",
    "sparsity_ratios = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "num_tokens = num_samples * sequence_length\n",
    "calib_data = get_fineweb_edu(num_tokens, sequence_length, tokenizer, train=True)\n",
    "prune_default(model, calib_data, sparsity_ratios, theta1=0.42, theta2=0.51, theta3=0.38, is_sparsegpt=False, device=torch.device(\"cuda:0\"), save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74cfeca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "930c4527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4718750000000001\n"
     ]
    }
   ],
   "source": [
    "target = 0.5\n",
    "sparsities = []\n",
    "possible_sparsities = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "for i in range(32):\n",
    "    # ensure not to exceed target sparsity\n",
    "    mask1 = [1 if (sum(sparsities[:i]) + s) / (32) <= target else 0 for s in possible_sparsities]\n",
    "    # ensure not to make target sparsity impossible to reach\n",
    "    mask2 = [1 if (sum(sparsities[:i]) + s + 0.9*(32 - (i+1)))/32 >= target else 0 for s in possible_sparsities]\n",
    "    mask = [m1*m2 for m1, m2 in zip(mask1, mask2)]\n",
    "    for j in range(len(mask)):\n",
    "        if mask[j] == 1:\n",
    "            sparsities.append(possible_sparsities[j])\n",
    "            break\n",
    "        \n",
    "print(sum(sparsities)/32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "932b6d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.19,\n",
       " 0.21,\n",
       " 0.23,\n",
       " 0.25,\n",
       " 0.27,\n",
       " 0.29000000000000004,\n",
       " 0.31,\n",
       " 0.33,\n",
       " 0.35,\n",
       " 0.37,\n",
       " 0.39,\n",
       " 0.41000000000000003,\n",
       " 0.43,\n",
       " 0.45,\n",
       " 0.47000000000000003,\n",
       " 0.49,\n",
       " 0.51,\n",
       " 0.53,\n",
       " 0.55,\n",
       " 0.5700000000000001,\n",
       " 0.5900000000000001,\n",
       " 0.61,\n",
       " 0.63,\n",
       " 0.65,\n",
       " 0.6699999999999999,\n",
       " 0.69,\n",
       " 0.71,\n",
       " 0.73,\n",
       " 0.75,\n",
       " 0.77,\n",
       " 0.79,\n",
       " 0.81]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = []\n",
    "S = 0.5\n",
    "b = 0.02\n",
    "for i in range(1, 33):\n",
    "    s.append(S - (b*(32-1))/2 + b*(i-1))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573171db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
