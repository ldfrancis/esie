// Inference Engine for LLMs for CPU/GPU
#include <iostream>
#include <vector>
#include <fstream>
#include <cstdint>
#include "models/llama.hpp"


int main(int argc, char* argv[]) {
    
    return 0;
}